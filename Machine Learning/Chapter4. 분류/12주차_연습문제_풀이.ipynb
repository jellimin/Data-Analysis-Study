{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d05ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris 데이터에 XGBoost 알고리즘(파이썬 래퍼를 이용)을 적용하고자 한다.\n",
    "\n",
    "# 1. 데이터를 불러오기 위해 아래 빈칸을 채워라.\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_iris()\n",
    "features = dataset.data\n",
    "labels = dataset.target\n",
    "\n",
    "iris_df = pd.DataFrame(data = features, columns = dataset.feature_names)\n",
    "iris_df['target'] = labels\n",
    "iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6686aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터를 학습, 검증, 예측 데이터셋으로 분할하기 위해 아래 빈칸을 채워라.\n",
    "# 단, 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터로 분리\n",
    "# 80%의 학습용 데이터를 다시 분리하여 90%는 학습용 데이터, 10%는 검증용 데이터로 분리\n",
    "\n",
    "X_features = iris_df.iloc[:, :-1]\n",
    "y_label = iris_df.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size = 0.2, random_state = 156)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da43097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. [파이썬 wrapper] 학습, 검증, 예측 데이터 세트를 DMatrix 객체인 dtr, dval, dtest로 변환하여라.\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtr = xgb.DMatrix(data = X_tr, label = y_tr)\n",
    "dval = xgb.DMatrix(data = X_val, label = y_val)\n",
    "dtest = xgb.DMatrix(data = X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b43f5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.52515\teval-logloss:0.57417\n",
      "[1]\ttrain-logloss:0.36788\teval-logloss:0.46432\n",
      "[2]\ttrain-logloss:0.21566\teval-logloss:0.35978\n",
      "[3]\ttrain-logloss:0.06264\teval-logloss:0.25678\n",
      "[4]\ttrain-logloss:-0.09918\teval-logloss:0.15104\n",
      "[5]\ttrain-logloss:-0.28442\teval-logloss:0.03327\n",
      "[6]\ttrain-logloss:-0.53285\teval-logloss:-0.11976\n",
      "[7]\ttrain-logloss:-1.15778\teval-logloss:-0.49001\n",
      "[8]\ttrain-logloss:-10.41252\teval-logloss:-5.86976\n",
      "[9]\ttrain-logloss:-10.46863\teval-logloss:-5.90053\n",
      "[10]\ttrain-logloss:-10.51995\teval-logloss:-5.93299\n",
      "[11]\ttrain-logloss:-10.57879\teval-logloss:-5.95932\n",
      "[12]\ttrain-logloss:-10.63902\teval-logloss:-5.98723\n",
      "[13]\ttrain-logloss:-11.59682\teval-logloss:-6.00996\n",
      "[14]\ttrain-logloss:-11.63979\teval-logloss:-6.03414\n",
      "[15]\ttrain-logloss:-11.97463\teval-logloss:-6.05612\n",
      "[16]\ttrain-logloss:-12.30921\teval-logloss:-6.07668\n",
      "[17]\ttrain-logloss:-12.33351\teval-logloss:-6.09593\n",
      "[18]\ttrain-logloss:-12.35613\teval-logloss:-6.11331\n",
      "[19]\ttrain-logloss:-12.37736\teval-logloss:-6.13024\n",
      "[20]\ttrain-logloss:-12.39725\teval-logloss:-6.14610\n",
      "[21]\ttrain-logloss:-12.41582\teval-logloss:-6.16044\n",
      "[22]\ttrain-logloss:-12.43319\teval-logloss:-6.17501\n",
      "[23]\ttrain-logloss:-12.44943\teval-logloss:-6.18785\n",
      "[24]\ttrain-logloss:-12.46478\teval-logloss:-6.20072\n",
      "[25]\ttrain-logloss:-12.47932\teval-logloss:-6.21266\n",
      "[26]\ttrain-logloss:-12.49286\teval-logloss:-6.22341\n",
      "[27]\ttrain-logloss:-12.50568\teval-logloss:-6.23415\n",
      "[28]\ttrain-logloss:-12.51764\teval-logloss:-6.24410\n",
      "[29]\ttrain-logloss:-12.52917\teval-logloss:-6.25283\n",
      "[30]\ttrain-logloss:-12.54002\teval-logloss:-6.26143\n",
      "[31]\ttrain-logloss:-12.55019\teval-logloss:-6.26996\n",
      "[32]\ttrain-logloss:-12.55986\teval-logloss:-6.27761\n",
      "[33]\ttrain-logloss:-12.56894\teval-logloss:-6.28522\n",
      "[34]\ttrain-logloss:-12.57759\teval-logloss:-6.29205\n",
      "[35]\ttrain-logloss:-12.58570\teval-logloss:-6.29884\n",
      "[36]\ttrain-logloss:-12.59344\teval-logloss:-6.30494\n",
      "[37]\ttrain-logloss:-12.60071\teval-logloss:-6.31102\n",
      "[38]\ttrain-logloss:-12.60755\teval-logloss:-6.31645\n",
      "[39]\ttrain-logloss:-12.61407\teval-logloss:-6.32190\n",
      "[40]\ttrain-logloss:-12.62018\teval-logloss:-6.32735\n",
      "[41]\ttrain-logloss:-12.62603\teval-logloss:-6.33228\n",
      "[42]\ttrain-logloss:-12.63157\teval-logloss:-6.33694\n",
      "[43]\ttrain-logloss:-12.63684\teval-logloss:-6.34132\n",
      "[44]\ttrain-logloss:-12.64182\teval-logloss:-6.34550\n",
      "[45]\ttrain-logloss:-12.64651\teval-logloss:-6.34970\n",
      "[46]\ttrain-logloss:-12.65100\teval-logloss:-6.35346\n",
      "[47]\ttrain-logloss:-12.65543\teval-logloss:-6.35702\n",
      "[48]\ttrain-logloss:-12.65943\teval-logloss:-6.36065\n",
      "[49]\ttrain-logloss:-12.66327\teval-logloss:-6.36386\n",
      "[50]\ttrain-logloss:-12.66708\teval-logloss:-6.36694\n",
      "[51]\ttrain-logloss:-12.67051\teval-logloss:-6.37010\n",
      "[52]\ttrain-logloss:-12.67395\teval-logloss:-6.37289\n",
      "[53]\ttrain-logloss:-12.67712\teval-logloss:-6.37522\n",
      "[54]\ttrain-logloss:-12.68013\teval-logloss:-6.37744\n",
      "[55]\ttrain-logloss:-12.68299\teval-logloss:-6.37954\n",
      "[56]\ttrain-logloss:-12.68571\teval-logloss:-6.38154\n",
      "[57]\ttrain-logloss:-12.68829\teval-logloss:-6.38343\n",
      "[58]\ttrain-logloss:-12.69075\teval-logloss:-6.38523\n",
      "[59]\ttrain-logloss:-12.69308\teval-logloss:-6.38694\n",
      "[60]\ttrain-logloss:-12.69530\teval-logloss:-6.38857\n",
      "[61]\ttrain-logloss:-12.69741\teval-logloss:-6.39011\n",
      "[62]\ttrain-logloss:-12.69942\teval-logloss:-6.39158\n",
      "[63]\ttrain-logloss:-12.70133\teval-logloss:-6.39297\n",
      "[64]\ttrain-logloss:-12.70314\teval-logloss:-6.39429\n",
      "[65]\ttrain-logloss:-12.70487\teval-logloss:-6.39555\n",
      "[66]\ttrain-logloss:-12.70652\teval-logloss:-6.39675\n",
      "[67]\ttrain-logloss:-12.70806\teval-logloss:-6.39789\n",
      "[68]\ttrain-logloss:-12.70955\teval-logloss:-6.39897\n",
      "[69]\ttrain-logloss:-12.71096\teval-logloss:-6.40026\n",
      "[70]\ttrain-logloss:-12.71229\teval-logloss:-6.40123\n",
      "[71]\ttrain-logloss:-12.71360\teval-logloss:-6.40241\n",
      "[72]\ttrain-logloss:-12.71481\teval-logloss:-6.40329\n",
      "[73]\ttrain-logloss:-12.71597\teval-logloss:-6.40436\n",
      "[74]\ttrain-logloss:-12.71709\teval-logloss:-6.40515\n",
      "[75]\ttrain-logloss:-12.71815\teval-logloss:-6.40612\n",
      "[76]\ttrain-logloss:-12.71879\teval-logloss:-6.40694\n",
      "[77]\ttrain-logloss:-12.71977\teval-logloss:-6.40785\n",
      "[78]\ttrain-logloss:-12.72036\teval-logloss:-6.40861\n",
      "[79]\ttrain-logloss:-12.72092\teval-logloss:-6.40934\n",
      "[80]\ttrain-logloss:-12.72146\teval-logloss:-6.41002\n",
      "[81]\ttrain-logloss:-12.72231\teval-logloss:-6.41085\n",
      "[82]\ttrain-logloss:-12.72311\teval-logloss:-6.41164\n",
      "[83]\ttrain-logloss:-12.72351\teval-logloss:-6.41174\n",
      "[84]\ttrain-logloss:-12.72426\teval-logloss:-6.41248\n",
      "[85]\ttrain-logloss:-12.72462\teval-logloss:-6.41257\n",
      "[86]\ttrain-logloss:-12.72532\teval-logloss:-6.41327\n",
      "[87]\ttrain-logloss:-12.72597\teval-logloss:-6.41376\n",
      "[88]\ttrain-logloss:-12.72637\teval-logloss:-6.41436\n",
      "[89]\ttrain-logloss:-12.72659\teval-logloss:-6.41507\n",
      "[90]\ttrain-logloss:-12.72720\teval-logloss:-6.41569\n",
      "[91]\ttrain-logloss:-12.72777\teval-logloss:-6.41611\n",
      "[92]\ttrain-logloss:-12.72796\teval-logloss:-6.41678\n",
      "[93]\ttrain-logloss:-12.72826\teval-logloss:-6.41682\n",
      "[94]\ttrain-logloss:-12.72873\teval-logloss:-6.41760\n",
      "[95]\ttrain-logloss:-12.72888\teval-logloss:-6.41782\n",
      "[96]\ttrain-logloss:-12.72931\teval-logloss:-6.41855\n",
      "[97]\ttrain-logloss:-12.72971\teval-logloss:-6.41924\n",
      "[98]\ttrain-logloss:-12.73009\teval-logloss:-6.41944\n",
      "[99]\ttrain-logloss:-12.73045\teval-logloss:-6.41963\n",
      "[100]\ttrain-logloss:-12.73080\teval-logloss:-6.41982\n",
      "[101]\ttrain-logloss:-12.73114\teval-logloss:-6.41999\n",
      "[102]\ttrain-logloss:-12.73146\teval-logloss:-6.42015\n",
      "[103]\ttrain-logloss:-12.73176\teval-logloss:-6.42031\n",
      "[104]\ttrain-logloss:-12.73206\teval-logloss:-6.42045\n",
      "[105]\ttrain-logloss:-12.73207\teval-logloss:-6.42042\n",
      "[106]\ttrain-logloss:-12.73234\teval-logloss:-6.42102\n",
      "[107]\ttrain-logloss:-12.73262\teval-logloss:-6.42116\n",
      "[108]\ttrain-logloss:-12.73289\teval-logloss:-6.42130\n",
      "[109]\ttrain-logloss:-12.73313\teval-logloss:-6.42186\n",
      "[110]\ttrain-logloss:-12.73306\teval-logloss:-6.42182\n",
      "[111]\ttrain-logloss:-12.73331\teval-logloss:-6.42195\n",
      "[112]\ttrain-logloss:-12.73352\teval-logloss:-6.42239\n",
      "[113]\ttrain-logloss:-12.73344\teval-logloss:-6.42236\n",
      "[114]\ttrain-logloss:-12.73345\teval-logloss:-6.42266\n",
      "[115]\ttrain-logloss:-12.73349\teval-logloss:-6.42294\n",
      "[116]\ttrain-logloss:-12.73381\teval-logloss:-6.42323\n",
      "[117]\ttrain-logloss:-12.73405\teval-logloss:-6.42338\n",
      "[118]\ttrain-logloss:-12.73429\teval-logloss:-6.42351\n",
      "[119]\ttrain-logloss:-12.73420\teval-logloss:-6.42348\n",
      "[120]\ttrain-logloss:-12.73448\teval-logloss:-6.42374\n",
      "[121]\ttrain-logloss:-12.73457\teval-logloss:-6.42392\n",
      "[122]\ttrain-logloss:-12.73450\teval-logloss:-6.42415\n",
      "[123]\ttrain-logloss:-12.73478\teval-logloss:-6.42441\n",
      "[124]\ttrain-logloss:-12.73503\teval-logloss:-6.42453\n",
      "[125]\ttrain-logloss:-12.73504\teval-logloss:-6.42459\n",
      "[126]\ttrain-logloss:-12.73505\teval-logloss:-6.42463\n",
      "[127]\ttrain-logloss:-12.73506\teval-logloss:-6.42468\n",
      "[128]\ttrain-logloss:-12.73503\teval-logloss:-6.42479\n",
      "[129]\ttrain-logloss:-12.73529\teval-logloss:-6.42515\n",
      "[130]\ttrain-logloss:-12.73519\teval-logloss:-6.42512\n",
      "[131]\ttrain-logloss:-12.73544\teval-logloss:-6.42535\n",
      "[132]\ttrain-logloss:-12.73544\teval-logloss:-6.42548\n",
      "[133]\ttrain-logloss:-12.73568\teval-logloss:-6.42577\n",
      "[134]\ttrain-logloss:-12.73582\teval-logloss:-6.42618\n",
      "[135]\ttrain-logloss:-12.73576\teval-logloss:-6.42614\n",
      "[136]\ttrain-logloss:-12.73573\teval-logloss:-6.42626\n",
      "[137]\ttrain-logloss:-12.73570\teval-logloss:-6.42630\n",
      "[138]\ttrain-logloss:-12.73593\teval-logloss:-6.42659\n",
      "[139]\ttrain-logloss:-12.73593\teval-logloss:-6.42659\n",
      "[140]\ttrain-logloss:-12.73614\teval-logloss:-6.42687\n",
      "[141]\ttrain-logloss:-12.73614\teval-logloss:-6.42692\n",
      "[142]\ttrain-logloss:-12.73611\teval-logloss:-6.42691\n",
      "[143]\ttrain-logloss:-12.73631\teval-logloss:-6.42718\n",
      "[144]\ttrain-logloss:-12.73632\teval-logloss:-6.42729\n",
      "[145]\ttrain-logloss:-12.73624\teval-logloss:-6.42748\n",
      "[146]\ttrain-logloss:-12.73645\teval-logloss:-6.42780\n",
      "[147]\ttrain-logloss:-12.73640\teval-logloss:-6.42777\n",
      "[148]\ttrain-logloss:-12.73660\teval-logloss:-6.42808\n",
      "[149]\ttrain-logloss:-12.73657\teval-logloss:-6.42808\n",
      "[150]\ttrain-logloss:-12.73677\teval-logloss:-6.42838\n",
      "[151]\ttrain-logloss:-12.73682\teval-logloss:-6.42836\n",
      "[152]\ttrain-logloss:-12.73674\teval-logloss:-6.42856\n",
      "[153]\ttrain-logloss:-12.73694\teval-logloss:-6.42886\n",
      "[154]\ttrain-logloss:-12.73691\teval-logloss:-6.42885\n",
      "[155]\ttrain-logloss:-12.73708\teval-logloss:-6.42909\n",
      "[156]\ttrain-logloss:-12.73708\teval-logloss:-6.42909\n",
      "[157]\ttrain-logloss:-12.73726\teval-logloss:-6.42937\n",
      "[158]\ttrain-logloss:-12.73730\teval-logloss:-6.42935\n",
      "[159]\ttrain-logloss:-12.73727\teval-logloss:-6.42935\n",
      "[160]\ttrain-logloss:-12.73724\teval-logloss:-6.42944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-logloss:-12.73738\teval-logloss:-6.42954\n",
      "[162]\ttrain-logloss:-12.73730\teval-logloss:-6.42952\n",
      "[163]\ttrain-logloss:-12.73747\teval-logloss:-6.42979\n",
      "[164]\ttrain-logloss:-12.73751\teval-logloss:-6.42977\n",
      "[165]\ttrain-logloss:-12.73765\teval-logloss:-6.42987\n",
      "[166]\ttrain-logloss:-12.73756\teval-logloss:-6.42985\n",
      "[167]\ttrain-logloss:-12.73773\teval-logloss:-6.43011\n",
      "[168]\ttrain-logloss:-12.73769\teval-logloss:-6.43009\n",
      "[169]\ttrain-logloss:-12.73785\teval-logloss:-6.43034\n",
      "[170]\ttrain-logloss:-12.73788\teval-logloss:-6.43032\n",
      "[171]\ttrain-logloss:-12.73801\teval-logloss:-6.43042\n",
      "[172]\ttrain-logloss:-12.73792\teval-logloss:-6.43039\n",
      "[173]\ttrain-logloss:-12.73791\teval-logloss:-6.43097\n",
      "[174]\ttrain-logloss:-12.73787\teval-logloss:-6.43095\n",
      "[175]\ttrain-logloss:-12.73789\teval-logloss:-6.43093\n",
      "[176]\ttrain-logloss:-12.73797\teval-logloss:-6.43121\n",
      "[177]\ttrain-logloss:-12.73812\teval-logloss:-6.43145\n",
      "[178]\ttrain-logloss:-12.73808\teval-logloss:-6.43143\n",
      "[179]\ttrain-logloss:-12.73822\teval-logloss:-6.43167\n",
      "[180]\ttrain-logloss:-12.73815\teval-logloss:-6.43184\n",
      "[181]\ttrain-logloss:-12.73829\teval-logloss:-6.43203\n",
      "[182]\ttrain-logloss:-12.73842\teval-logloss:-6.43225\n",
      "[183]\ttrain-logloss:-12.73845\teval-logloss:-6.43223\n",
      "[184]\ttrain-logloss:-12.73842\teval-logloss:-6.43222\n",
      "[185]\ttrain-logloss:-12.73840\teval-logloss:-6.43221\n",
      "[186]\ttrain-logloss:-12.73854\teval-logloss:-6.43243\n",
      "[187]\ttrain-logloss:-12.73856\teval-logloss:-6.43241\n",
      "[188]\ttrain-logloss:-12.73852\teval-logloss:-6.43239\n",
      "[189]\ttrain-logloss:-12.73859\teval-logloss:-6.43264\n",
      "[190]\ttrain-logloss:-12.73865\teval-logloss:-6.43289\n",
      "[191]\ttrain-logloss:-12.73878\teval-logloss:-6.43309\n",
      "[192]\ttrain-logloss:-12.73871\teval-logloss:-6.43325\n",
      "[193]\ttrain-logloss:-12.73883\teval-logloss:-6.43345\n",
      "[194]\ttrain-logloss:-12.73885\teval-logloss:-6.43343\n",
      "[195]\ttrain-logloss:-12.73881\teval-logloss:-6.43341\n",
      "[196]\ttrain-logloss:-12.73893\teval-logloss:-6.43360\n",
      "[197]\ttrain-logloss:-12.73895\teval-logloss:-6.43358\n",
      "[198]\ttrain-logloss:-12.73893\teval-logloss:-6.43357\n",
      "[199]\ttrain-logloss:-12.73905\teval-logloss:-6.43376\n",
      "[200]\ttrain-logloss:-12.73911\teval-logloss:-6.43398\n",
      "[201]\ttrain-logloss:-12.73904\teval-logloss:-6.43395\n",
      "[202]\ttrain-logloss:-12.73900\teval-logloss:-6.43393\n",
      "[203]\ttrain-logloss:-12.73910\teval-logloss:-6.43411\n",
      "[204]\ttrain-logloss:-12.73910\teval-logloss:-6.43370\n",
      "[205]\ttrain-logloss:-12.73916\teval-logloss:-6.43392\n",
      "[206]\ttrain-logloss:-12.73927\teval-logloss:-6.43432\n",
      "[207]\ttrain-logloss:-12.73927\teval-logloss:-6.43416\n",
      "[208]\ttrain-logloss:-12.73936\teval-logloss:-6.43432\n",
      "[209]\ttrain-logloss:-12.73936\teval-logloss:-6.43394\n",
      "[210]\ttrain-logloss:-12.73954\teval-logloss:-6.43420\n",
      "[211]\ttrain-logloss:-12.73960\teval-logloss:-6.43451\n",
      "[212]\ttrain-logloss:-12.73965\teval-logloss:-6.43480\n",
      "[213]\ttrain-logloss:-12.73982\teval-logloss:-6.43505\n",
      "[214]\ttrain-logloss:-12.73986\teval-logloss:-6.43532\n",
      "[215]\ttrain-logloss:-12.73985\teval-logloss:-6.43515\n",
      "[216]\ttrain-logloss:-12.73984\teval-logloss:-6.43499\n",
      "[217]\ttrain-logloss:-12.73982\teval-logloss:-6.43484\n",
      "[218]\ttrain-logloss:-12.73999\teval-logloss:-6.43507\n",
      "[219]\ttrain-logloss:-12.74004\teval-logloss:-6.43535\n",
      "[220]\ttrain-logloss:-12.74008\teval-logloss:-6.43561\n",
      "[221]\ttrain-logloss:-12.74007\teval-logloss:-6.43581\n",
      "[222]\ttrain-logloss:-12.74023\teval-logloss:-6.43604\n",
      "[223]\ttrain-logloss:-12.74027\teval-logloss:-6.43628\n",
      "[224]\ttrain-logloss:-12.74035\teval-logloss:-6.43647\n",
      "[225]\ttrain-logloss:-12.74031\teval-logloss:-6.43660\n",
      "[226]\ttrain-logloss:-12.74030\teval-logloss:-6.43643\n",
      "[227]\ttrain-logloss:-12.74034\teval-logloss:-6.43660\n",
      "[228]\ttrain-logloss:-12.74049\teval-logloss:-6.43682\n",
      "[229]\ttrain-logloss:-12.74055\teval-logloss:-6.43698\n",
      "[230]\ttrain-logloss:-12.74052\teval-logloss:-6.43710\n",
      "[231]\ttrain-logloss:-12.74049\teval-logloss:-6.43722\n",
      "[232]\ttrain-logloss:-12.74053\teval-logloss:-6.43737\n",
      "[233]\ttrain-logloss:-12.74057\teval-logloss:-6.43760\n",
      "[234]\ttrain-logloss:-12.74060\teval-logloss:-6.43782\n",
      "[235]\ttrain-logloss:-12.74064\teval-logloss:-6.43803\n",
      "[236]\ttrain-logloss:-12.74068\teval-logloss:-6.43825\n",
      "[237]\ttrain-logloss:-12.74073\teval-logloss:-6.43829\n",
      "[238]\ttrain-logloss:-12.74074\teval-logloss:-6.43850\n",
      "[239]\ttrain-logloss:-12.74081\teval-logloss:-6.43854\n",
      "[240]\ttrain-logloss:-12.74083\teval-logloss:-6.43872\n",
      "[241]\ttrain-logloss:-12.74084\teval-logloss:-6.43883\n",
      "[242]\ttrain-logloss:-12.74088\teval-logloss:-6.43903\n",
      "[243]\ttrain-logloss:-12.74092\teval-logloss:-6.43907\n",
      "[244]\ttrain-logloss:-12.74094\teval-logloss:-6.43909\n",
      "[245]\ttrain-logloss:-12.74097\teval-logloss:-6.43928\n",
      "[246]\ttrain-logloss:-12.74103\teval-logloss:-6.43943\n",
      "[247]\ttrain-logloss:-12.74108\teval-logloss:-6.43956\n",
      "[248]\ttrain-logloss:-12.74109\teval-logloss:-6.43965\n",
      "[249]\ttrain-logloss:-12.74110\teval-logloss:-6.43973\n",
      "[250]\ttrain-logloss:-12.74114\teval-logloss:-6.43985\n",
      "[251]\ttrain-logloss:-12.74115\teval-logloss:-6.43993\n",
      "[252]\ttrain-logloss:-12.74120\teval-logloss:-6.44004\n",
      "[253]\ttrain-logloss:-12.74120\teval-logloss:-6.44011\n",
      "[254]\ttrain-logloss:-12.74124\teval-logloss:-6.44021\n",
      "[255]\ttrain-logloss:-12.74125\teval-logloss:-6.44027\n",
      "[256]\ttrain-logloss:-12.74129\teval-logloss:-6.44037\n",
      "[257]\ttrain-logloss:-12.74129\teval-logloss:-6.44043\n",
      "[258]\ttrain-logloss:-12.74133\teval-logloss:-6.44052\n",
      "[259]\ttrain-logloss:-12.74133\teval-logloss:-6.44057\n",
      "[260]\ttrain-logloss:-12.74139\teval-logloss:-6.44067\n",
      "[261]\ttrain-logloss:-12.74139\teval-logloss:-6.44072\n",
      "[262]\ttrain-logloss:-12.74144\teval-logloss:-6.44081\n",
      "[263]\ttrain-logloss:-12.74146\teval-logloss:-6.44075\n",
      "[264]\ttrain-logloss:-12.74146\teval-logloss:-6.44080\n",
      "[265]\ttrain-logloss:-12.74152\teval-logloss:-6.44084\n",
      "[266]\ttrain-logloss:-12.74157\teval-logloss:-6.44092\n",
      "[267]\ttrain-logloss:-12.74156\teval-logloss:-6.44096\n",
      "[268]\ttrain-logloss:-12.74157\teval-logloss:-6.44090\n",
      "[269]\ttrain-logloss:-12.74163\teval-logloss:-6.44093\n",
      "[270]\ttrain-logloss:-12.74164\teval-logloss:-6.44086\n",
      "[271]\ttrain-logloss:-12.74163\teval-logloss:-6.44091\n",
      "[272]\ttrain-logloss:-12.74167\teval-logloss:-6.44098\n",
      "[273]\ttrain-logloss:-12.74171\teval-logloss:-6.44110\n",
      "[274]\ttrain-logloss:-12.74171\teval-logloss:-6.44103\n",
      "[275]\ttrain-logloss:-12.74170\teval-logloss:-6.44107\n",
      "[276]\ttrain-logloss:-12.74171\teval-logloss:-6.44100\n",
      "[277]\ttrain-logloss:-12.74174\teval-logloss:-6.44111\n",
      "[278]\ttrain-logloss:-12.74170\teval-logloss:-6.44104\n",
      "[279]\ttrain-logloss:-12.74174\teval-logloss:-6.44102\n",
      "[280]\ttrain-logloss:-12.74175\teval-logloss:-6.44096\n",
      "[281]\ttrain-logloss:-12.74178\teval-logloss:-6.44107\n",
      "[282]\ttrain-logloss:-12.74182\teval-logloss:-6.44105\n",
      "[283]\ttrain-logloss:-12.74186\teval-logloss:-6.44103\n",
      "[284]\ttrain-logloss:-12.74187\teval-logloss:-6.44098\n",
      "[285]\ttrain-logloss:-12.74184\teval-logloss:-6.44080\n",
      "[286]\ttrain-logloss:-12.74181\teval-logloss:-6.44074\n",
      "[287]\ttrain-logloss:-12.74182\teval-logloss:-6.44080\n",
      "[288]\ttrain-logloss:-12.74179\teval-logloss:-6.44064\n",
      "[289]\ttrain-logloss:-12.74177\teval-logloss:-6.44048\n",
      "[290]\ttrain-logloss:-12.74175\teval-logloss:-6.44034\n",
      "[291]\ttrain-logloss:-12.74177\teval-logloss:-6.44033\n",
      "[292]\ttrain-logloss:-12.74175\teval-logloss:-6.44023\n",
      "[293]\ttrain-logloss:-12.74177\teval-logloss:-6.44023\n",
      "[294]\ttrain-logloss:-12.74178\teval-logloss:-6.44020\n",
      "[295]\ttrain-logloss:-12.74181\teval-logloss:-6.44020\n",
      "[296]\ttrain-logloss:-12.74182\teval-logloss:-6.44018\n",
      "[297]\ttrain-logloss:-12.74185\teval-logloss:-6.44017\n",
      "[298]\ttrain-logloss:-12.74188\teval-logloss:-6.44015\n",
      "[299]\ttrain-logloss:-12.74189\teval-logloss:-6.44013\n",
      "[300]\ttrain-logloss:-12.74190\teval-logloss:-6.44003\n",
      "[301]\ttrain-logloss:-12.74187\teval-logloss:-6.43998\n",
      "[302]\ttrain-logloss:-12.74188\teval-logloss:-6.43996\n",
      "[303]\ttrain-logloss:-12.74190\teval-logloss:-6.43996\n",
      "[304]\ttrain-logloss:-12.74188\teval-logloss:-6.43994\n",
      "[305]\ttrain-logloss:-12.74188\teval-logloss:-6.43992\n",
      "[306]\ttrain-logloss:-12.74191\teval-logloss:-6.43994\n",
      "[307]\ttrain-logloss:-12.74190\teval-logloss:-6.43998\n",
      "[308]\ttrain-logloss:-12.74194\teval-logloss:-6.44010\n",
      "[309]\ttrain-logloss:-12.74193\teval-logloss:-6.44014\n",
      "[310]\ttrain-logloss:-12.74193\teval-logloss:-6.44018\n",
      "[311]\ttrain-logloss:-12.74193\teval-logloss:-6.44023\n",
      "[312]\ttrain-logloss:-12.74195\teval-logloss:-6.44014\n",
      "[313]\ttrain-logloss:-12.74198\teval-logloss:-6.44026\n",
      "[314]\ttrain-logloss:-12.74197\teval-logloss:-6.44030\n",
      "[315]\ttrain-logloss:-12.74200\teval-logloss:-6.44041\n",
      "[316]\ttrain-logloss:-12.74199\teval-logloss:-6.44045\n",
      "[317]\ttrain-logloss:-12.74200\teval-logloss:-6.44049\n",
      "[318]\ttrain-logloss:-12.74197\teval-logloss:-6.44038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[319]\ttrain-logloss:-12.74194\teval-logloss:-6.44028\n",
      "[320]\ttrain-logloss:-12.74191\teval-logloss:-6.44019\n",
      "[321]\ttrain-logloss:-12.74188\teval-logloss:-6.44010\n",
      "[322]\ttrain-logloss:-12.74187\teval-logloss:-6.43997\n",
      "[323]\ttrain-logloss:-12.74191\teval-logloss:-6.44009\n",
      "[324]\ttrain-logloss:-12.74194\teval-logloss:-6.44020\n",
      "[325]\ttrain-logloss:-12.74193\teval-logloss:-6.44024\n",
      "[326]\ttrain-logloss:-12.74190\teval-logloss:-6.44015\n",
      "[327]\ttrain-logloss:-12.74190\teval-logloss:-6.44005\n"
     ]
    }
   ],
   "source": [
    "# 4. [파이썬 wrapper] 설정한 하이퍼 파라미터와 early stopping 파라미터를 전달하여 학습하기 위해 아래 빈칸을 채워라.\n",
    "# 조기 중단을 위한 횟수는 50회로 설정\n",
    "\n",
    "params = {'max_depth':3,\n",
    "          'eta':0.05,\n",
    "          'eval_metric':'logloss'}\n",
    "num_rounds = 400\n",
    "\n",
    "eval_list = [(dtr, 'train'), (dval, 'eval')]\n",
    "\n",
    "xgb_model = xgb.train(params = params, dtrain = dtr, num_boost_round = num_rounds,\n",
    "                      early_stopping_rounds = 50, evals = eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9536618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.001e+00 1.000e-03 9.910e-01 2.025e+00 1.000e-03 1.273e+00 2.129e+00\n",
      " 2.025e+00 1.000e-03 1.909e+00]\n"
     ]
    }
   ],
   "source": [
    "# 5. [파이썬 wrapper] predict() 함수를 적용하여 예측 확률값을 반환하고, 이를 다시 예측값으로 반환하기 위한 빈칸을 채워라.\n",
    "pred_probs = xgb_model.predict(dtest)\n",
    "print(np.round(pred_probs[:10], 3)) # predict() 수행 결과값을 10개만 표시\n",
    "\n",
    "# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 probs에 저장\n",
    "# 이 때, 리스트 내포 내의 if-else 문을 사용하여라.\n",
    "preds = [1 if x > 0.5 else 0 for x in pred_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2501611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# 6. 정확도를 이용한 예측 평가를 위해 빈 칸을 채워라.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
