{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2422380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 누락 데이터 처리\n",
    "# 머신러닝 등 데이터 분석의 정확도는 분석 데이터의 품질에 의해 좌우\n",
    "# 데이터 품질을 높이기 위해서는 누락 데이터, 중복 데이터 등 오류를 수정하고 분석 목적에 맞게 변형하는 과정이 필요\n",
    "# 유효한 데이터 값이 존재하지 않는 누락 데이터를 NaN(Not a Number)\n",
    "# 머신러닝 분석 모형에 데이터를 입력하기 전에는 [누락 데이터를 제거 / 다른 적절한 값으로 대체]하는 과정이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64b1a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 누락 데이터 확인\n",
    "# titanic 데이터의 deck에 NaN의 값이 있음\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55397f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# 1. info() 메소드로 요약정보 출력하면 각 열에 속하는 데이터 중 유효한 값의 개수를 보여줌\n",
    "# 총 891개의 데이터, deck 열에는 203개의 유효한 범주형 데이터가 있음.\n",
    "# 따라서 688개의 누락데이터가 있음\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395755ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN    688\n",
      "C       59\n",
      "B       47\n",
      "D       33\n",
      "E       32\n",
      "A       15\n",
      "F       13\n",
      "G        4\n",
      "Name: deck, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. value_counts()를 이용해서 누락데이터 개수 파악 가능\n",
    "\n",
    "# DataFrame 객체.count() : 각 열의 데이터 개수를 시리즈 객체로 표시\n",
    "# DataFrame 객체.value_counts() : 해당 열의 고유값의 개수를 세는데 사용 (dropna = False를 사용하면 Na의 개수도 세어줌)\n",
    "\n",
    "nan_deck = df['deck'].value_counts(dropna = False)\n",
    "print(nan_deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc24494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass    sex    age  sibsp  parch   fare  embarked  class  \\\n",
      "0     False   False  False  False  False  False  False     False  False   \n",
      "1     False   False  False  False  False  False  False     False  False   \n",
      "2     False   False  False  False  False  False  False     False  False   \n",
      "3     False   False  False  False  False  False  False     False  False   \n",
      "4     False   False  False  False  False  False  False     False  False   \n",
      "\n",
      "     who  adult_male   deck  embark_town  alive  alone  \n",
      "0  False       False   True        False  False  False  \n",
      "1  False       False  False        False  False  False  \n",
      "2  False       False   True        False  False  False  \n",
      "3  False       False  False        False  False  False  \n",
      "4  False       False   True        False  False  False  \n",
      "\n",
      "\n",
      "   survived  pclass   sex   age  sibsp  parch  fare  embarked  class   who  \\\n",
      "0      True    True  True  True   True   True  True      True   True  True   \n",
      "1      True    True  True  True   True   True  True      True   True  True   \n",
      "2      True    True  True  True   True   True  True      True   True  True   \n",
      "3      True    True  True  True   True   True  True      True   True  True   \n",
      "4      True    True  True  True   True   True  True      True   True  True   \n",
      "\n",
      "   adult_male   deck  embark_town  alive  alone  \n",
      "0        True  False         True   True   True  \n",
      "1        True   True         True   True   True  \n",
      "2        True  False         True   True   True  \n",
      "3        True   True         True   True   True  \n",
      "4        True  False         True   True   True  \n",
      "\n",
      "\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "deck           3\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. 누락데이터를 직접적으로 찾는 방법\n",
    "# isnull() : 누락 데이터면 True를 반환하고, 유효한 데이터가 존재하면 False를 반환\n",
    "# notnull() : 유효한 데이터가 존재하면 True를 반환, 누락 데이터면 False를 반환\n",
    "\n",
    "# 누락데이터인 경우 True\n",
    "print(df.head().isnull())\n",
    "print('\\n')\n",
    "\n",
    "# 유효한 데이터인 경우 True\n",
    "print(df.head().notnull())\n",
    "print('\\n')\n",
    "\n",
    "# 각 열의 누락 데이터의 개수 구하기 (axis = 0) / 행으로 구할 경우에는 axis = 1\n",
    "print(df.head().isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dc85afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived :  0\n",
      "pclass :  0\n",
      "sex :  0\n",
      "age :  177\n",
      "sibsp :  0\n",
      "parch :  0\n",
      "fare :  0\n",
      "embarked :  2\n",
      "class :  0\n",
      "who :  0\n",
      "adult_male :  0\n",
      "deck :  688\n",
      "embark_town :  2\n",
      "alive :  0\n",
      "alone :  0\n",
      "\n",
      "\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive',\n",
      "       'alone'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
      "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
      "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
      "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
      "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
      "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
      "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
      "885         0       3  female  39.0      0      5  29.1250        Q   Third   \n",
      "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
      "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
      "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
      "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
      "\n",
      "       who  adult_male deck  embark_town alive  alone  \n",
      "0      man        True  NaN  Southampton    no  False  \n",
      "1    woman       False    C    Cherbourg   yes  False  \n",
      "2    woman       False  NaN  Southampton   yes   True  \n",
      "3    woman       False    C  Southampton   yes  False  \n",
      "4      man        True  NaN  Southampton    no   True  \n",
      "..     ...         ...  ...          ...   ...    ...  \n",
      "885  woman       False  NaN   Queenstown    no  False  \n",
      "886    man        True  NaN  Southampton    no   True  \n",
      "887  woman       False    B  Southampton   yes   True  \n",
      "889    man        True    C    Cherbourg   yes   True  \n",
      "890    man        True  NaN   Queenstown    no   True  \n",
      "\n",
      "[714 rows x 15 columns]\n",
      "714\n"
     ]
    }
   ],
   "source": [
    "# 누락 데이터 제거\n",
    "# 누락 데이터가 들어 있는 열 또는 행을 삭제하는 방법\n",
    "# 열 삭제 -> 특성(변수) 제거 / 행 삭제 -> 대상의 관측값(레코드) 삭제\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# for 반복문으로 각 열의 NaN 개수 계산하기\n",
    "missing_df = df.isnull()\n",
    "for col in missing_df.columns:\n",
    "    missing_count = missing_df[col].value_counts()\n",
    "    \n",
    "    try:\n",
    "        print(col, ': ', missing_count[True])\n",
    "    except:\n",
    "        print(col, ': ', 0)\n",
    "        \n",
    "print('\\n')\n",
    "        \n",
    "# deck 데이터가 688명이나 누락되어 있음\n",
    "# 따라서, deck 열의 누락 데이터를 삭제하여 분석에서 제외하는 것이 의미있음\n",
    "# dropna() 메소드에 thresh = 500 옵션을 적용하여 NaN 값을 500개 이상 갖는 모든 열을 삭제\n",
    "\n",
    "df_thresh = df.dropna(axis = 1, thresh = 500)\n",
    "print(df_thresh.columns) # deck 열이 삭제된 것을 확인할 수 있음\n",
    "print('\\n')\n",
    "\n",
    "# 승객의 나이 데이터는 177명이 누락되어 있음\n",
    "# 만약, 승객의 나이가 데이터 분석의 중요한 변수라면 나이 데이터가 없는 승객의 레코드를 제거하는 것이 좋음\n",
    "# dropna() 모소드에 subset을 age 열로 한정하면, age 열의 행 중 NaN값이 있는 모든 행을 삭제\n",
    "# how = 'any' (하나라도 존재하면 삭제) / how ='all' (모든 데이터가 NaN일 경우 삭제)\n",
    "\n",
    "df_age = df.dropna(subset = ['age'], how = 'any', axis = 0)\n",
    "print(len(df_age)) # 714개의 행이 남음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8940a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22.0\n",
      "1    38.0\n",
      "2    26.0\n",
      "3    35.0\n",
      "4    35.0\n",
      "5     NaN\n",
      "6    54.0\n",
      "7     2.0\n",
      "8    27.0\n",
      "9    14.0\n",
      "Name: age, dtype: float64\n",
      "\n",
      "\n",
      "0    22.000000\n",
      "1    38.000000\n",
      "2    26.000000\n",
      "3    35.000000\n",
      "4    35.000000\n",
      "5    29.699118\n",
      "6    54.000000\n",
      "7     2.000000\n",
      "8    27.000000\n",
      "9    14.000000\n",
      "Name: age, dtype: float64\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n",
      "\n",
      "\n",
      "Southampton\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829    Southampton\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 누락 데이터 치환 fillna()\n",
    "# 데이터셋의 품질을 높일 목적으로 누락 데이터를 무작정 삭제한다면 어렵게 수집한 데이터를 활용하지 못함\n",
    "# 데이터 분석의 정확도는 데이터 품질 외에도 데이터 양에도 상당한 영향을 받음\n",
    "# 누락 데이터를 바꿔서 대체할 값으로는 데이터의 분포와 특성을 잘 나타낼 수 있는 평균값, 최빈값 등 활용\n",
    "# 판다스 fillna() 메소드로 편하게 처리 가능, 새로운 객체 반환하기 때문에 inplace = True 옵션 필요\n",
    "\n",
    "\n",
    "# age 열에 누락 데이터를 평균값으로 치환하기\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "print(df['age'].head(10)) # 5행에 NaN 값 존재\n",
    "print('\\n')\n",
    "\n",
    "# age 열의 NaN 값을 다른 나이 데이터의 평균으로 변경하기 (중간값을 사용하려면 median() 메소드를 적용)\n",
    "mean_age = df['age'].mean(axis = 0) # mean() 메소드 사용 시 NaN을 제외한 평균값을 구해줌\n",
    "df['age'].fillna(mean_age, inplace = True)\n",
    "\n",
    "print(df['age'].head(10))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 승선도시를 나타내는 embark_town 열에 있는 NaN을 승객들이 가장 많이 승선한 도시의 이름을 찾아 치환하기\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# embark_town 열의 829행의 NaN 데이터 출력\n",
    "print(df['embark_town'][825:830])\n",
    "print('\\n')\n",
    "\n",
    "# embark_town 열의 NaN 값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기\n",
    "most_freq = df['embark_town'].value_counts(dropna = False).idxmax() # idxmax() : 최대값을 가지는 인덱스 라벨을 출력\n",
    "print(most_freq)\n",
    "print('\\n')\n",
    "\n",
    "df['embark_town'].fillna(most_freq, inplace = True)\n",
    "\n",
    "print(df['embark_town'][825:830])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 누락 데이터가 NaN으로 표시되지 않은 경우\n",
    "# 누락 데이터가 NaN이 아닌 숫자 0이나 문자 -, ?와 같은 값으로 입력되기도 함\n",
    "# 따라서, 누락 데이터를 다루기 위해서는 replace() 메소드를 활용하여 Numpy에서 지원하는 np.nan으로 변경해주는 것이 좋다\n",
    "\n",
    "# 사용법 예시 \n",
    "# import numpy as np\n",
    "# df.replace('?', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d834c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829     Queenstown\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 특성상 서로 이웃하고 있는 데이터끼리 유사성을 가질 가능성이 높음\n",
    "# 이럴 때에는 앞이나 뒤에서 이웃하고 있는 값으로 치환해주는 것이 좋음\n",
    "# fillna() 메소드에 method = 'ffill' (직전 행 값으로 대체) / 'bfill' (다음 행 값으로 대체)를 사용하면 됨\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "print(df['embark_town'][825:830])\n",
    "print('\\n')\n",
    "\n",
    "df['embark_town'].fillna(method = 'ffill', inplace = True)\n",
    "print(df['embark_town'][825:830])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 중복 데이터 처리\n",
    "# 데이터프레임에서 각 행은 분석 대상이 갖고 있는 모든 속성에 대한 관측값을 뜻함\n",
    "# 하나의 데이터셋에서 동일한 관측값이 2개 이상 중복되는 경우, 삭제해야 함\n",
    "# 동일한 대상이 중복으로 존재하는 것이므로 분석 결과를 왜곡하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff2a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: c2, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# 중복 데이터 확인\n",
    "# duplicated() 메소드를 이용하여 행의 레코드가 중복되는지 여부를 확인\n",
    "# 전에 나온 행들과 비교하여 중복되는 행이면 True, 처음 나오는 행에 대해서는 False를 반환\n",
    "\n",
    "\n",
    "# 1. 데이터프레임 중복 데이터 확인\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'c1':['a', 'a', 'b', 'a', 'b'],\n",
    "                   'c2':[1, 1, 1, 2, 2],\n",
    "                   'c3':[1, 1, 2, 2, 2]})\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터프레임 전체 행 데이터 중에서 중복값 찾기\n",
    "df_dup = df.duplicated()\n",
    "print(df_dup)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 2. 시리즈 객체 중복 데이터 확인\n",
    "col_dup = df['c2'].duplicated()\n",
    "print(col_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2aa59212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n"
     ]
    }
   ],
   "source": [
    "# 중복 데이터 제거\n",
    "# 중복 데이터를 제거하기 위해서는 drop_duplicates() 메소드가 있음\n",
    "# 중복된 행을 제거하고 고유한 관측값을 가진 행들만 남김\n",
    "# 원본 객체를 변경하기 위해서는 inplace = True라는 옵션을 추가\n",
    "\n",
    "\n",
    "# 1. 데이터프레임 중복 데이터 제거\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'c1':['a', 'a', 'b', 'a', 'b'],\n",
    "                   'c2':[1, 1, 1, 2, 2],\n",
    "                   'c3':[1, 1, 2, 2, 2]})\n",
    "\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터 프레임에서 중복 행 제거\n",
    "df2 = df.drop_duplicates()\n",
    "print(df2)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 2. 데이터프레임의 특정 열에 대한 중복 데이터 제거\n",
    "# drop_duplicates() 메소드의 subset 옵션에 열 이름 리스트를 전달할 수 있음\n",
    "\n",
    "df3 = df.drop_duplicates(subset = ['c2', 'c3'])\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61186831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터 표준화\n",
    "# 여러 곳에서 수집한 자료들은 단위 선택, 대소문자 구분, 약칭 활용 등 여러 가지 원인에 의해 다양한 형태로 표현\n",
    "# 이처럼 동일한 대상을 표현하는 방법에 차이가 있으면 분석의 정확도는 현저히 낮아짐. 따라서 데이터 포맷을 일관성 있게 표준화하는 작업 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63e42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "\n",
      "\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name       kpl  \n",
      "0       1  chevrolet chevelle malibu  7.652571  \n",
      "1       1          buick skylark 320  6.377143  \n",
      "2       1         plymouth satellite  7.652571  \n",
      "\n",
      "\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name   kpl  \n",
      "0       1  chevrolet chevelle malibu  7.65  \n",
      "1       1          buick skylark 320  6.38  \n",
      "2       1         plymouth satellite  7.65  \n"
     ]
    }
   ],
   "source": [
    "# 3-1. 단위 환산\n",
    "# 특히, 외국 데이터를 가져오면 국내에서 잘 사용하지 않는 도량형 단위를 사용하는 경우가 많음\n",
    "# 영미권에서 주로 사용하는 마일, 야드, 온스 등이 있는데 한국에서 사용하는 미터, 평, 그램 등으로 변환하는 것이 좋음\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "              'acceleration', 'model year', 'origin', 'name']\n",
    "print(df.head(3))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# mpg(mile per gallon)을 kpl(kilometer per liter)로 변환 (mpg_to_kpl = 0.425)\n",
    "mpg_to_kpl = 1.60934 / 3.78541\n",
    "\n",
    "# mpg 열에 0.425를 곱한 결과를 새로운 열(kpl)에 추가\n",
    "df['kpl'] = df['mpg'] * mpg_to_kpl\n",
    "print(df.head(3))\n",
    "print('\\n')\n",
    "\n",
    "# kpl 열을 소수점 아래 둘째자리에서 반올림\n",
    "df['kpl'] = df['kpl'].round(2)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f71061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg             float64\n",
      "cylinders         int64\n",
      "displacement    float64\n",
      "horsepower       object\n",
      "weight          float64\n",
      "acceleration    float64\n",
      "model year        int64\n",
      "origin            int64\n",
      "name             object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horsepower    398 non-null    object \n",
      " 4   weight        398 non-null    float64\n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   model year    398 non-null    int64  \n",
      " 7   origin        398 non-null    int64  \n",
      " 8   name          398 non-null    object \n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 28.1+ KB\n",
      "None\n",
      "\n",
      "\n",
      "['130.0' '165.0' '150.0' '140.0' '198.0' '220.0' '215.0' '225.0' '190.0'\n",
      " '170.0' '160.0' '95.00' '97.00' '85.00' '88.00' '46.00' '87.00' '90.00'\n",
      " '113.0' '200.0' '210.0' '193.0' '?' '100.0' '105.0' '175.0' '153.0'\n",
      " '180.0' '110.0' '72.00' '86.00' '70.00' '76.00' '65.00' '69.00' '60.00'\n",
      " '80.00' '54.00' '208.0' '155.0' '112.0' '92.00' '145.0' '137.0' '158.0'\n",
      " '167.0' '94.00' '107.0' '230.0' '49.00' '75.00' '91.00' '122.0' '67.00'\n",
      " '83.00' '78.00' '52.00' '61.00' '93.00' '148.0' '129.0' '96.00' '71.00'\n",
      " '98.00' '115.0' '53.00' '81.00' '79.00' '120.0' '152.0' '102.0' '108.0'\n",
      " '68.00' '58.00' '149.0' '89.00' '63.00' '48.00' '66.00' '139.0' '103.0'\n",
      " '125.0' '133.0' '138.0' '135.0' '142.0' '77.00' '62.00' '132.0' '84.00'\n",
      " '64.00' '74.00' '116.0' '82.00']\n",
      "\n",
      "\n",
      "float64\n",
      "\n",
      "\n",
      "[1 3 2]\n",
      "\n",
      "\n",
      "['USA' 'JPN' 'EU']\n",
      "\n",
      "\n",
      "object\n",
      "\n",
      "\n",
      "category\n",
      "\n",
      "\n",
      "394    82\n",
      "216    77\n",
      "183    76\n",
      "Name: model year, dtype: int64\n",
      "\n",
      "\n",
      "259    78\n",
      "54     71\n",
      "193    76\n",
      "Name: model year, dtype: category\n",
      "Categories (13, int64): [70, 71, 72, 73, ..., 79, 80, 81, 82]\n"
     ]
    }
   ],
   "source": [
    "# 3-2. 자료형 변환\n",
    "# 숫자가 문자열(object)로 저장된 경우에 숫자형(int 혹은 float)으로 변환해야 함\n",
    "# 먼저, dtypes 속성을 사용하여 데이터프레임을 구성하는 각 열의 자료형을 확인\n",
    "# dtypes 대신 info() 메소드를 사용해도 확인할 수 있음\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "              'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "print(df.dtypes) # horsepower은 숫자형, model year, origin은 카테고리를 나타내는 범주형이 적절\n",
    "print('\\n')\n",
    "print(df.info())\n",
    "print('\\n')\n",
    "\n",
    "# 1. horsepower을 숫자형으로 변환\n",
    "# 엔진 출력이 문자열로 저장된 이유 \n",
    "# 열의 고유값을 출력해보자\n",
    "print(df['horsepower'].unique()) # 고유값 중 문자 '?'로 섞여 데이터프레임으로 변환하는 과정에서 문자열로 인식됨\n",
    "print('\\n') \n",
    "\n",
    "# horsepower 열의 문자열 ?를 NaN으로 변환\n",
    "# dropna 메소드로 NaN값이 들어있는 모든 행 삭제\n",
    "# astype('float') 명령을 이용하여 문자열을 실수형으로 변환\n",
    "\n",
    "import numpy as np\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float') # astype()은 데이터프레임의 타입을 바꿀 때 사용\n",
    "\n",
    "print(df['horsepower'].dtypes)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# 2. origin을 범주형으로 변환\n",
    "# origin은 1, 2, 3 대신 실제 국가 이름으로 바꾸어주면 문자열을 나타내는 object로 자동 변경\n",
    "print(df['origin'].unique())\n",
    "print('\\n')\n",
    "\n",
    "df['origin'].replace({1:'USA', 2:'EU', 3:'JPN'}, inplace = True)\n",
    "print(df['origin'].unique())\n",
    "print('\\n')\n",
    "print(df['origin'].dtypes) # 문자열로 바뀜\n",
    "print('\\n')\n",
    "\n",
    "# origin은 3개의 국가이름이 반복되므로, 이를 범주형 데이터로 표현하는 것이 효율적\n",
    "# astype('category') 메소드를 이용하여 범주형 데이터로 변환\n",
    "# 범주형을 다시 문자열로 변환하려면 astype('str') 메소드를 사용\n",
    "df['origin'] = df['origin'].astype('category')\n",
    "print(df['origin'].dtypes)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 3. model_year을 범주형으로 변환\n",
    "# 연도는 시간적인 순서는 의미 있으나 숫자의 상대적인 크기는 별 의미가 없음\n",
    "# 따라서, 데이터는 숫자 형태를 갖더라도 자료형은 범주형으로 표현하는 것이 적절\n",
    "print(df['model year'].sample(3)) # 무작위로 3개의 행을 출력하여 선택\n",
    "print('\\n')\n",
    "\n",
    "df['model year'] = df['model year'].astype('category')\n",
    "print(df['model year'].sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cbf074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46.         107.33333333 168.66666667 230.        ]\n",
      "\n",
      "\n",
      "    horsepower hp_bin\n",
      "0        130.0   보통출력\n",
      "1        165.0   보통출력\n",
      "2        150.0   보통출력\n",
      "3        150.0   보통출력\n",
      "4        140.0   보통출력\n",
      "5        198.0    고출력\n",
      "6        220.0    고출력\n",
      "7        215.0    고출력\n",
      "8        225.0    고출력\n",
      "9        190.0    고출력\n",
      "10       170.0    고출력\n",
      "11       160.0   보통출력\n",
      "12       150.0   보통출력\n",
      "13       225.0    고출력\n",
      "14        95.0    저출력\n"
     ]
    }
   ],
   "source": [
    "# 4. 범주형(카테고리) 데이터 처리\n",
    "\n",
    "# 4-1. 구간 분할\n",
    "\n",
    "# 넘파이 histogram() 이용하여 각 구간에 속하는 값의 개수 / 경계값 구하기\n",
    "# 판다스 cut() 함수 이용\n",
    "\n",
    "# 데이터 분석 알고리즘에 따라서는 연속 데이터를 그대로 사용하기 보다는 일정한 구간(bin)으로 나눠서 분석하는 것이 효율적인 경우가 있음\n",
    "# 구간분할(binning) : 연속 변수를 일정한 구간으로 나누고, 각 구간을 범주형 이산 변수로 변환하는 과정\n",
    "# 판다스 cut() 함수를 이용하면 연속 데이터를 여러 구간으로 나누고, 범주형 데이터로 변환 가능\n",
    "\n",
    "# horsepower은 숫자로 표현하는 대신 저출력 / 보통출력 / 고출력 등 구간으로 나누어 표시하는 것이 효율적일 수 있음\n",
    "# 3개의 구간으로 나누기 위해서는 총 4개의 경계값이 필요\n",
    "# 경계값을 구하는 방법 : NumPy 라이브러리의 histogram() 함수를 활용, 나누려는 구간 개수를 bins 옵션에 입력하면 각 구간에 속하는 값의 개수(count)와 경계값 리스트(bin_dividers)를 반환\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "              'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "# horsepower 열의 누락 데이터('?')를 삭제하고 실수형으로 변환\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "# np.histogram 함수로 3개의 bin으로 구분할 경계값의 리스트 구하기\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins = 3)\n",
    "print(bin_dividers)\n",
    "print('\\n')\n",
    "\n",
    "# 3개의 bin에 이름 지정\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "\n",
    "# pd.cut 함수로 각 데이터를 3개의 bin에 할당\n",
    "df['hp_bin'] = pd.cut(x = df['horsepower'], # 데이터 배열\n",
    "                      bins = bin_dividers, # 경계값 리스트\n",
    "                      labels = bin_names, # bin 이름\n",
    "                      include_lowest = True) # 각 구간의 낮은 경계값 포함 여부\n",
    "\n",
    "print(df[['horsepower', 'hp_bin']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e347d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저출력  보통출력  고출력\n",
      "0     0     1    0\n",
      "1     0     1    0\n",
      "2     0     1    0\n",
      "3     0     1    0\n",
      "4     0     1    0\n",
      "5     0     0    1\n",
      "6     0     0    1\n",
      "7     0     0    1\n",
      "8     0     0    1\n",
      "9     0     0    1\n",
      "10    0     0    1\n",
      "11    0     1    0\n",
      "12    0     1    0\n",
      "13    0     0    1\n",
      "14    1     0    0\n"
     ]
    }
   ],
   "source": [
    "# 4-2. 더미 변수\n",
    "\n",
    "# 판다스 get_dummies() 함수 이용\n",
    "\n",
    "# 앞에서 horsepower 열의 숫자형 연속 데이터를 hp_bin 열의 범주형 데이터로 변환\n",
    "# 하지만, 카테고리를 나타내는 범주형 데이터를 머신러닝 알고리즘에 바로 사용할 수 없는 경우가 있어, 컴퓨터가 인식 가능한 입력값으로 변환해야 함\n",
    "# 이런 경우, 숫자 0 또는 1로 표현되는 더미 변수(dummy variable)을 사용\n",
    "# 여기서, 0과 1은 크고 작음을 나타내지 않고, 어떤 특성이 있는지 없는지 여부만을 표시\n",
    "# 이처럼 범주형 데이터를 컴퓨터가 인식할 수 있도록 숫자 0,1로만 구성한다고 해서 원핫인코딩이라고도 부름\n",
    "\n",
    "# 판다스 get_dummies() 함수를 사용하면, 범주형 변수의 모든 고유값을 각각 새로운 더미 변수로 변환\n",
    "# hp_bin 열의 고유값 3개가 각각 새로운 더미변수 열의 이름이 됨\n",
    "# 각 더미 변수가 본래 속해 있던 행에는 1이 입력되고, 속하지 않았던 다른 행에는 0이 입력됨\n",
    "\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins = 3)\n",
    "\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "\n",
    "df['hp_bin'] = pd.cut(x = df['horsepower'],\n",
    "                      bins = bin_dividers,\n",
    "                      labels = bin_names,\n",
    "                      include_lowest = True)\n",
    "\n",
    "# hp_bin 열의 범주형 데이터를 더미 변수로 변환\n",
    "horsepower_dummies = pd.get_dummies(df['hp_bin'])\n",
    "print(horsepower_dummies.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de23b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 1)\t1.0\n",
      "  (12, 1)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# sklearn 라이브러리를 이용해서 원핫인코딩을 편하게 처리할 수 있음\n",
    "# hp_bin 열에 들어 있는 범주형 데이터를 0, 1을 원소로 갖는 원핫벡터로 변환\n",
    "# 결과는 선형대수학에서 정의하는 희소행렬(sparse matrix)로 정리됨\n",
    "# 예제에서는 1차원 벡터를 2차원 행렬로 변환한 후 다시 희소행렬로 변환함\n",
    "# 희소행렬은 (행, 열) 좌표와 값의 형태로 정리\n",
    "# 예를 들어 (0,1) 1.0은 0행 1열의 위치의 값은 1.0을 의미함\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 전처리를 위한 encoder 객체 만들기\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder()\n",
    "\n",
    "# label_encoder로 문자열 범주를 숫자형 범주로 변환\n",
    "onehot_labeled = label_encoder.fit_transform(df['hp_bin'].head(15))\n",
    "print(onehot_labeled)\n",
    "print(type(onehot_labeled))\n",
    "print('\\n')\n",
    "\n",
    "# 2차원 행렬로 변경\n",
    "onehot_reshaped = onehot_labeled.reshape(len(onehot_labeled), 1)\n",
    "print(onehot_reshaped)\n",
    "print(type(onehot_reshaped))\n",
    "print('\\n')\n",
    "\n",
    "# 희소행렬로 변환\n",
    "onehot_fitted = onehot_encoder.fit_transform(onehot_reshaped)\n",
    "print(onehot_fitted)\n",
    "print(type(onehot_fitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05450e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    392.000000\n",
      "mean     104.469388\n",
      "std       38.491160\n",
      "min       46.000000\n",
      "25%       75.000000\n",
      "50%       93.500000\n",
      "75%      126.000000\n",
      "max      230.000000\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "0    0.565217\n",
      "1    0.717391\n",
      "2    0.652174\n",
      "3    0.652174\n",
      "4    0.608696\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean       0.454215\n",
      "std        0.167353\n",
      "min        0.200000\n",
      "25%        0.326087\n",
      "50%        0.406522\n",
      "75%        0.547826\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean     104.469388\n",
      "std       38.491160\n",
      "min       46.000000\n",
      "25%       75.000000\n",
      "50%       93.500000\n",
      "75%      126.000000\n",
      "max      230.000000\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "0    0.456522\n",
      "1    0.646739\n",
      "2    0.565217\n",
      "3    0.565217\n",
      "4    0.510870\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean       0.317768\n",
      "std        0.209191\n",
      "min        0.000000\n",
      "25%        0.157609\n",
      "50%        0.258152\n",
      "75%        0.434783\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 5. 정규화\n",
    "# 숫자 데이터의 상대적인 크기 차이를 제거할 필요가 있음\n",
    "# 각 열에 속하는 데이터 값을 동일한 크기 기준으로 나눈 비율로 나타내는 것을 정규화라고 함\n",
    "# 정규화 과정을 거친 데이터의 범위는 0~1 또는 -1 ~ 1이 됨\n",
    "\n",
    "\n",
    "\n",
    "# 1. 각 열의 데이터를 해당 열의 최대값(의 절대값)으로 나누는 방법\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "              'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "# horsepower 열의 누락 데이터('?')를 삭제하고 실수형으로 변환\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "# horsepower 열의 통계 요약 정보로 최대값(max) 확인\n",
    "print(df.horsepower.describe())\n",
    "print('\\n')\n",
    "\n",
    "# horsepower 열의 최대값의 절대값으로 모든 데이터를 나눠서 저장\n",
    "df.horsepower = df.horsepower / abs(df.horsepower.max())\n",
    "print(df.horsepower.head())\n",
    "print('\\n')\n",
    "print(df.horsepower.describe())\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# 2. 각 열의 데이터 중에서 최대값과 최소값을 뺀 값으로 나누는 방법\n",
    "# 각 열의 데이터에서 해당 열의 최소값을 뺀 값을 분자로 하고, 해당 열의 최대값과 최소값의 차를 분모로 하는 수를 계산하면 가장 큰 값 역시 1이 됨\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "              'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "# horsepower 열의 누락 데이터('?')를 삭제하고 실수형으로 변환\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "# horsepower 열의 통계 요약 정보로 최대값, 최소값 확인\n",
    "print(df.horsepower.describe())\n",
    "print('\\n')\n",
    "\n",
    "# horsepower 열의 최대값의 절대값으로 모든 데이터를 나눠서 저장\n",
    "min_x = df.horsepower - df.horsepower.min()\n",
    "min_max = df.horsepower.max() - df.horsepower.min()\n",
    "df.horsepower = min_x / min_max\n",
    "\n",
    "print(df.horsepower.head())\n",
    "print('\\n')\n",
    "print(df.horsepower.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3816a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 시계열 데이터\n",
    "# 주식, 환율 등 금융 데이터를 다루기 위해 개발된 판다스는 시계열 데이터를 다루는 여러 가지 유용한 기능 제공\n",
    "# 특히, 시계열 데이터를 데이터프레임의 행 인덱스로 사용하면, 시간으로 기록된 데이터를 분석하는 것이 매우 편리\n",
    "# 판다스의 시간 표시 방식 중 자주 이용되는 2가지 유형 : 특정 시점 기록하는 Timestamp / 두 시점 사이의 일정한 기간을 나타내는 Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "843c4939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume\n",
      "0  2018-07-02  10100  10850  10900  10000  137977\n",
      "1  2018-06-29  10700  10550  10900   9990  170253\n",
      "2  2018-06-28  10400  10900  10950  10150  155769\n",
      "3  2018-06-27  10900  10800  11050  10500  133548\n",
      "4  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    20 non-null     object\n",
      " 1   Close   20 non-null     int64 \n",
      " 2   Start   20 non-null     int64 \n",
      " 3   High    20 non-null     int64 \n",
      " 4   Low     20 non-null     int64 \n",
      " 5   Volume  20 non-null     int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "\n",
      "\n",
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Date      20 non-null     object        \n",
      " 1   Close     20 non-null     int64         \n",
      " 2   Start     20 non-null     int64         \n",
      " 3   High      20 non-null     int64         \n",
      " 4   Low       20 non-null     int64         \n",
      " 5   Volume    20 non-null     int64         \n",
      " 6   new_Date  20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 1.2+ KB\n",
      "None\n",
      "\n",
      "\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "\n",
      "\n",
      "            Close  Start   High    Low  Volume\n",
      "new_Date                                      \n",
      "2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20 entries, 2018-07-02 to 2018-06-01\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Close   20 non-null     int64\n",
      " 1   Start   20 non-null     int64\n",
      " 2   High    20 non-null     int64\n",
      " 3   Low     20 non-null     int64\n",
      " 4   Volume  20 non-null     int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 960.0 bytes\n",
      "None\n",
      "\n",
      "\n",
      "DatetimeIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='period[D]')\n",
      "PeriodIndex(['2019-01', '2020-03', '2021-06'], dtype='period[M]')\n",
      "PeriodIndex(['2019', '2020', '2021'], dtype='period[A-DEC]')\n"
     ]
    }
   ],
   "source": [
    "# 6-1. 다른 자료형을 시계열 객체로 변환\n",
    "# 우리가 접하는 많은 시간 데이터들은 별도의 시간 자료형(파이썬 datetime 라이브러리 등)으로 기록되지 않고, 문자열 or 숫자로 저장되는 경우가 많음\n",
    "# 판다스는 다른 자료형으로 저장된 시간 데이터를 판다스 시계열 객체인 Timestamp로 변환하는 함수 제공\n",
    "\n",
    "\n",
    "# 1. 문자열을 Timestamp로 변환\n",
    "# 판다스 to_datetime() 함수를 사용하면 문자열 등 다른 자료형을 판다스 Timestamp를 나타내는 datetime64 자료형으로 변환 가능\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./stock-data.csv')\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info()) # Date 열의 자료형이 문자열(object)임을 알 수 있음\n",
    "print('\\n')\n",
    "\n",
    "df['new_Date'] = pd.to_datetime(df['Date']) # 문자열 데이터를 판다스 Timestamp로 변환하여 새로운 열로 추가\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())\n",
    "print('\\n')\n",
    "print(type(df['new_Date'][0])) # 해당 열의 개별 원소 데이터를 type() 함수로 확인하면 Timestamp 객체임을 알 수 있음\n",
    "print('\\n')\n",
    "\n",
    "# 시계열 인덱스 지정\n",
    "# 시계열 값을 행 인덱스로 지정하면 판다스는 DatetimeIndex로 저장함 (원래는 RangeIndex임)\n",
    "# 시계열 인덱스 클래스를 지원하기 때문에 시간 순서에 맞춰 인덱싱, 슬라이싱 하기가 편함\n",
    "df.set_index('new_Date', inplace = True)\n",
    "df.drop('Date', axis = 1, inplace = True)\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Timestamp를 Period로 변환\n",
    "# 판다스 to_period() 함수를 이용하면 일정한 기간을 나타내는 Period 객체로 Timestamp 객체를 변환할 수 있음\n",
    "# freq 옵션에 기준이 되는 기간을 설정\n",
    "# freq 옵션을 'D'로 지정할 경우 1일의 기간, 'M'으로 지정할 경우 1개월의 기간, 'A'로 지정할 경우 1년의 기간을 나타냄\n",
    "import pandas as pd\n",
    "\n",
    "dates = ['2019-01-01', '2020-03-01', '2021-06-01']\n",
    "\n",
    "# 문자열의 배열을 판다스 Timestamp로 변환\n",
    "ts_dates = pd.to_datetime(dates)\n",
    "print(ts_dates)\n",
    "print('\\n')\n",
    "\n",
    "# Timestamp를 Period로 변환\n",
    "pr_day = ts_dates.to_period(freq = 'D')\n",
    "print(pr_day)\n",
    "pr_month = ts_dates.to_period(freq = 'M')\n",
    "print(pr_month)\n",
    "pr_year = ts_dates.to_period(freq = 'A')\n",
    "print(pr_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ed984d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:00+09:00', '2019-02-01 00:00:00+09:00',\n",
      "               '2019-03-01 00:00:00+09:00', '2019-04-01 00:00:00+09:00',\n",
      "               '2019-05-01 00:00:00+09:00', '2019-06-01 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='MS')\n",
      "\n",
      "\n",
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-02-28 00:00:00+09:00',\n",
      "               '2019-03-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-05-31 00:00:00+09:00', '2019-06-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='M')\n",
      "\n",
      "\n",
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-07-31 00:00:00+09:00', '2019-10-31 00:00:00+09:00',\n",
      "               '2020-01-31 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='3M')\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01', '2019-02', '2019-03'], dtype='period[M]')\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01-01 00:00', '2019-01-01 01:00', '2019-01-01 02:00'], dtype='period[H]')\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01-01 00:00', '2019-01-01 02:00', '2019-01-01 04:00'], dtype='period[2H]')\n"
     ]
    }
   ],
   "source": [
    "# 6-2. 시계열 데이터 만들기\n",
    "\n",
    "# 1. Timestamp 배열\n",
    "# 판다스 date_range() 함수를 사용하면 여러 개의 날짜가 들어 있는 배열 형태의 시계열 데이터를 만들 수 있음\n",
    "# 파이썬 range() 함수로 숫자 배열을 만드는 것과 비슷\n",
    "import pandas as pd\n",
    "\n",
    "ts_ms = pd.date_range(start = '2019-01-01', # 날짜 범위 시작\n",
    "                      end = None, # 날짜 범위 끝\n",
    "                      periods = 6, # 생성할 timestamp 개수\n",
    "                      freq = 'MS', # 시간 간격 (MS : 월의 시작일)\n",
    "                      tz = 'Asia/Seoul') # 시간대(timezone)\n",
    "\n",
    "print(ts_ms)\n",
    "print('\\n')\n",
    "\n",
    "# 시간 간격을 다르게 설정\n",
    "# freq = 'M'으로 입력하면 월의 마지막 날짜를 생성\n",
    "# freq = '3M'으로 하며 3개월 간격의 마지막 날짜를 나타냄\n",
    "ts_me = pd.date_range('2019-01-01', periods = 6,\n",
    "                      freq = 'M',\n",
    "                      tz = 'Asia/Seoul')\n",
    "print(ts_me)\n",
    "print('\\n')\n",
    "\n",
    "ts_3m = pd.date_range('2019-01-01', periods = 5,\n",
    "                      freq = '3M',\n",
    "                      tz = 'Asia/Seoul')\n",
    "print(ts_3m)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Period 배열\n",
    "# 판다스 period_range() 함수는 여러 개의 기간이 들어 있는 시계열 데이터를 만듦\n",
    "# PeriodIndex의 원소 '2019-01'은 2019년 1월의 전체 기간을 의미\n",
    "import pandas as pd\n",
    "pr_m = pd.period_range(start = '2019-01-01', # 날짜 범위 시작\n",
    "                       end = None, # 날짜 범위 끝\n",
    "                       periods = 3, # 생성할 period 개수\n",
    "                       freq = 'M') # 기간의 길이(M : 월)\n",
    "print(pr_m)\n",
    "print('\\n')\n",
    "\n",
    "# 기간을 다르게 설정\n",
    "# freq = 'H' 옵션은 1시간 간격을 나타냄\n",
    "# freq = '2H' 옵션은 2시간 간격을 나타냄\n",
    "pr_h = pd.period_range(start = '2019-01-01',\n",
    "                       end = None,\n",
    "                       periods = 3,\n",
    "                       freq = 'H')\n",
    "print(pr_h)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "pr_2h = pd.period_range(start = '2019-01-01',\n",
    "                        end = None,\n",
    "                        periods = 3,\n",
    "                        freq = '2H')\n",
    "print(pr_2h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d878d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "\n",
      "\n",
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  Day\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7    2\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   26\n",
      "\n",
      "\n",
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  \\\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7   \n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   \n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   \n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   \n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   \n",
      "\n",
      "   Day Date_yr   Date_m  \n",
      "0    2    2018  2018-07  \n",
      "1   29    2018  2018-06  \n",
      "2   28    2018  2018-06  \n",
      "3   27    2018  2018-06  \n",
      "4   26    2018  2018-06  \n",
      "\n",
      "\n",
      "               Date  Close  Start   High    Low  Volume   new_Date  Year  \\\n",
      "Date_m                                                                     \n",
      "2018-07  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018   \n",
      "2018-06  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018   \n",
      "2018-06  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018   \n",
      "2018-06  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018   \n",
      "2018-06  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018   \n",
      "\n",
      "         Month  Day Date_yr  \n",
      "Date_m                       \n",
      "2018-07      7    2    2018  \n",
      "2018-06      6   29    2018  \n",
      "2018-06      6   28    2018  \n",
      "2018-06      6   27    2018  \n",
      "2018-06      6   26    2018  \n"
     ]
    }
   ],
   "source": [
    "# 6-3. 시계열 데이터 활용\n",
    "\n",
    "# 1. 날짜 데이터 분리\n",
    "\n",
    "# 연 - 월 -일 날짜 데이터에서 일부를 분리하여 추출 가능\n",
    "# dt 속성을 이용하여 개별적으로 추출 가능\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./stock-data.csv')\n",
    "\n",
    "df['new_Date'] = pd.to_datetime(df['Date']) # 문자열인 날짜 데이터를 판다스 Timestamp로 변환하여 새로운 열로 추가\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "\n",
    "# dt 속성을 이용하여 new_Date 열의 연-월-일 정보를 년, 월, 일로 구분\n",
    "df['Year'] = df['new_Date'].dt.year\n",
    "df['Month'] = df['new_Date'].dt.month\n",
    "df['Day'] = df['new_Date'].dt.day\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "\n",
    "# Timestamp 객체를 Period 객체로 변환하는 to_period() 메소드를 적용하여, 연-월-일 중에서 연-월 또는 연도를 추출\n",
    "df['Date_yr'] = df['new_Date'].dt.to_period(freq = 'A')\n",
    "df['Date_m'] = df['new_Date'].dt.to_period(freq = 'M')\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "\n",
    "# 추출한 날짜 정보를 데이터프레임의 행 인덱스로 지정 가능\n",
    "df.set_index('Date_m', inplace = True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2de92831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "DatetimeIndex(['2018-07-02', '2018-06-29', '2018-06-28', '2018-06-27',\n",
      "               '2018-06-26', '2018-06-25', '2018-06-22', '2018-06-21',\n",
      "               '2018-06-20', '2018-06-19', '2018-06-18', '2018-06-15',\n",
      "               '2018-06-14', '2018-06-12', '2018-06-11', '2018-06-08',\n",
      "               '2018-06-07', '2018-06-05', '2018-06-04', '2018-06-01'],\n",
      "              dtype='datetime64[ns]', name='new_Date', freq=None)\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "\n",
      "\n",
      "            Start   High\n",
      "new_Date                \n",
      "2018-07-02  10850  10900\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Date, Close, Start, High, Low, Volume]\n",
      "Index: []\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "time_delta                                                \n",
      "180 days    2018-06-28  10400  10900  10950  10150  155769\n",
      "181 days    2018-06-27  10900  10800  11050  10500  133548\n",
      "182 days    2018-06-26  10800  10900  11000  10700   63039\n",
      "183 days    2018-06-25  11150  11400  11450  11000   55519\n",
      "186 days    2018-06-22  11300  11250  11450  10750  134805\n",
      "187 days    2018-06-21  11200  11350  11750  11200  133002\n",
      "188 days    2018-06-20  11550  11200  11600  10900  308596\n",
      "189 days    2018-06-19  11300  11850  11950  11300  180656\n"
     ]
    }
   ],
   "source": [
    "# 2. 날짜 인덱스 활용\n",
    "# Timestamp로 구성된 열을 행 인덱스로 지정하면 DatetimeIndex라는 고유 속성으로 변환됨\n",
    "# 마찬가지로, Period로 구성된 열을 행 인덱스로 지정하면 PeriodIndex라는 속성을 가짐\n",
    "# 이와 같은 날짜 인덱스를 활용하면 시계열 데이터에 대한 인덱싱과 슬라이싱이 편함\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('new_Date', inplace = True)\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.index)\n",
    "print('\\n')\n",
    "\n",
    "# 날짜 인덱스를 사용하는 장점은 연 - 월 - 일 중에서 내가 필요로 하는 레벨을 선택적으로 인덱싱 가능\n",
    "# 연도, 연-월, 연-월-일을 기준으로 선택 가능\n",
    "# 날짜 범위로 슬라이싱 추출도 가능\n",
    "df_y = df.loc['2018'] # 2018년인 행만 추출\n",
    "print(df_y.head())\n",
    "print('\\n')\n",
    "\n",
    "df_ym = df.loc['2018-07']\n",
    "print(df_ym)\n",
    "print('\\n')\n",
    "\n",
    "df_ym_cols = df.loc['2018-07', 'Start':'High'] # 원소 선택\n",
    "print(df_ym_cols)\n",
    "print('\\n')\n",
    "\n",
    "df_ymd = df.loc['2018-07-02']\n",
    "print(df_ymd)\n",
    "print('\\n')\n",
    "\n",
    "df_ymd_range = df.loc['2018-06-25':'2018-06-20'] # 날짜 범위 지정, 빈 데이터프레임 출력, 오류\n",
    "print(df_ymd_range)\n",
    "print('\\n')\n",
    "\n",
    "# Timestamp 객체로 표시된 두 날짜 사이의 시간 간격 구하기\n",
    "# 어떤 날짜로부터 경과한 날짜를 계산하여 행 인덱스로 지정하고, 데이터를 선택\n",
    "\n",
    "# 시간 간격 계산, 최근 180일 ~ 189일 사이의 값들만 선택\n",
    "today = pd.to_datetime('2018-12-25') # 기준일 생성\n",
    "df['time_delta'] = today - df.index # 날짜 차이 계산\n",
    "df.set_index('time_delta', inplace = True) # 행 인덱스로 지정\n",
    "df_180 = df.loc['180 days':'189 days']\n",
    "print(df_180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
